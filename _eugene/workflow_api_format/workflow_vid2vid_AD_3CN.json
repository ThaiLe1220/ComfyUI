{
  "10": {
    "inputs": {
      "video": "/home/ubuntu/Desktop/Eugene/ComfyUI/input/__mixkit__/1552-720.mp4",
      "force_rate": 0,
      "force_size": "Disabled",
      "custom_width": 512,
      "custom_height": 512,
      "frame_load_cap": 120,
      "skip_first_frames": 0,
      "select_every_nth": 2
    },
    "class_type": "VHS_LoadVideoPath",
    "_meta": {
      "title": "Load Video (Path) ğŸ¥ğŸ…¥ğŸ…—ğŸ…¢"
    }
  },
  "14": {
    "inputs": {
      "size": 512,
      "interpolation_mode": "nearest exact",
      "image": [
        "10",
        0
      ]
    },
    "class_type": "JWImageResizeByShorterSide",
    "_meta": {
      "title": "Image Resize by Shorter Side"
    }
  },
  "15": {
    "inputs": {
      "image": [
        "14",
        0
      ]
    },
    "class_type": "ImagePass",
    "_meta": {
      "title": "ImagePass"
    }
  },
  "16": {
    "inputs": {
      "purge_cache": true,
      "purge_models": true,
      "anything": [
        "15",
        0
      ]
    },
    "class_type": "LayerUtility: PurgeVRAM",
    "_meta": {
      "title": "LayerUtility: Purge VRAM"
    }
  },
  "17": {
    "inputs": {
      "image": [
        "15",
        0
      ]
    },
    "class_type": "Get resolution [Crystools]",
    "_meta": {
      "title": "ğŸª› Get resolution"
    }
  },
  "37": {
    "inputs": {
      "preprocessor": "LineArtPreprocessor",
      "resolution": 512,
      "image": [
        "15",
        0
      ]
    },
    "class_type": "AIO_Preprocessor",
    "_meta": {
      "title": "AIO Aux Preprocessor"
    }
  },
  "38": {
    "inputs": {
      "ckpt_name": "realisticVisionV60B1_v51HyperVAE.safetensors",
      "vae_name": "vae-ft-mse-840000-ema-pruned.safetensors",
      "clip_skip": -1,
      "lora_name": "lcm/SD1.5/pytorch_lora_weights.safetensors",
      "lora_model_strength": 0.5,
      "lora_clip_strength": 0,
      "positive": "(realistic photo), A brown and white dog smiles at the camera, then looks to the side before staring at the camera again",
      "negative": "(nsfw:1.25), (nipples:1.25), (low quality, worst quality:1.2), low-resolution, lowres, jpeg artifacts, compression artifacts, poorly drawn, downsampling, aliasing, distorted, pixelated, fake, hyper, glitch, distortion, text, watermark, signature, user name, artist name, moir pattern, blurry, glossy, ugly, twisted, excessive, exaggerated pose, exaggerated limbs, grainy, duplicate, error, beginner, overexposed, high-contrast, bad-contrast, selfie, handy, phone, embedding:badhandv4, naked, nude, deformed iris, deformed pupils, semi-realistic, cgi, 3d, render, sketch, cartoon, drawing, anime, mutated hands and fingers:1.4, deformed, distorted, disfigured:1.3, bad anatomy, wrong anatomy, extra limb, missing limb, floating limbs, disconnected limbs, mutation, mutated, disgusting, amputation\n",
      "token_normalization": "mean",
      "weight_interpretation": "A1111",
      "empty_latent_width": [
        "17",
        0
      ],
      "empty_latent_height": [
        "17",
        1
      ],
      "batch_size": 1,
      "lora_stack": [
        "40",
        0
      ],
      "cnet_stack": [
        "53",
        0
      ]
    },
    "class_type": "Efficient Loader",
    "_meta": {
      "title": "Efficient Loader"
    }
  },
  "40": {
    "inputs": {
      "input_mode": "advanced",
      "lora_count": 1,
      "lora_name_1": "add_detail.safetensors",
      "lora_wt_1": 1,
      "model_str_1": 0.5,
      "clip_str_1": 0,
      "lora_name_2": "Smoooth-0-1.safetensors",
      "lora_wt_2": 1,
      "model_str_2": 1,
      "clip_str_2": 1,
      "lora_name_3": "None",
      "lora_wt_3": 1,
      "model_str_3": 1,
      "clip_str_3": 1,
      "lora_name_4": "None",
      "lora_wt_4": 1,
      "model_str_4": 1,
      "clip_str_4": 1,
      "lora_name_5": "None",
      "lora_wt_5": 1,
      "model_str_5": 1,
      "clip_str_5": 1,
      "lora_name_6": "None",
      "lora_wt_6": 1,
      "model_str_6": 1,
      "clip_str_6": 1,
      "lora_name_7": "None",
      "lora_wt_7": 1,
      "model_str_7": 1,
      "clip_str_7": 1,
      "lora_name_8": "None",
      "lora_wt_8": 1,
      "model_str_8": 1,
      "clip_str_8": 1,
      "lora_name_9": "None",
      "lora_wt_9": 1,
      "model_str_9": 1,
      "clip_str_9": 1,
      "lora_name_10": "None",
      "lora_wt_10": 1,
      "model_str_10": 1,
      "clip_str_10": 1,
      "lora_name_11": "None",
      "lora_wt_11": 1,
      "model_str_11": 1,
      "clip_str_11": 1,
      "lora_name_12": "None",
      "lora_wt_12": 1,
      "model_str_12": 1,
      "clip_str_12": 1,
      "lora_name_13": "None",
      "lora_wt_13": 1,
      "model_str_13": 1,
      "clip_str_13": 1,
      "lora_name_14": "None",
      "lora_wt_14": 1,
      "model_str_14": 1,
      "clip_str_14": 1,
      "lora_name_15": "None",
      "lora_wt_15": 1,
      "model_str_15": 1,
      "clip_str_15": 1,
      "lora_name_16": "None",
      "lora_wt_16": 1,
      "model_str_16": 1,
      "clip_str_16": 1,
      "lora_name_17": "None",
      "lora_wt_17": 1,
      "model_str_17": 1,
      "clip_str_17": 1,
      "lora_name_18": "None",
      "lora_wt_18": 1,
      "model_str_18": 1,
      "clip_str_18": 1,
      "lora_name_19": "None",
      "lora_wt_19": 1,
      "model_str_19": 1,
      "clip_str_19": 1,
      "lora_name_20": "None",
      "lora_wt_20": 1,
      "model_str_20": 1,
      "clip_str_20": 1,
      "lora_name_21": "None",
      "lora_wt_21": 1,
      "model_str_21": 1,
      "clip_str_21": 1,
      "lora_name_22": "None",
      "lora_wt_22": 1,
      "model_str_22": 1,
      "clip_str_22": 1,
      "lora_name_23": "None",
      "lora_wt_23": 1,
      "model_str_23": 1,
      "clip_str_23": 1,
      "lora_name_24": "None",
      "lora_wt_24": 1,
      "model_str_24": 1,
      "clip_str_24": 1,
      "lora_name_25": "None",
      "lora_wt_25": 1,
      "model_str_25": 1,
      "clip_str_25": 1,
      "lora_name_26": "None",
      "lora_wt_26": 1,
      "model_str_26": 1,
      "clip_str_26": 1,
      "lora_name_27": "None",
      "lora_wt_27": 1,
      "model_str_27": 1,
      "clip_str_27": 1,
      "lora_name_28": "None",
      "lora_wt_28": 1,
      "model_str_28": 1,
      "clip_str_28": 1,
      "lora_name_29": "None",
      "lora_wt_29": 1,
      "model_str_29": 1,
      "clip_str_29": 1,
      "lora_name_30": "None",
      "lora_wt_30": 1,
      "model_str_30": 1,
      "clip_str_30": 1,
      "lora_name_31": "None",
      "lora_wt_31": 1,
      "model_str_31": 1,
      "clip_str_31": 1,
      "lora_name_32": "None",
      "lora_wt_32": 1,
      "model_str_32": 1,
      "clip_str_32": 1,
      "lora_name_33": "None",
      "lora_wt_33": 1,
      "model_str_33": 1,
      "clip_str_33": 1,
      "lora_name_34": "None",
      "lora_wt_34": 1,
      "model_str_34": 1,
      "clip_str_34": 1,
      "lora_name_35": "None",
      "lora_wt_35": 1,
      "model_str_35": 1,
      "clip_str_35": 1,
      "lora_name_36": "None",
      "lora_wt_36": 1,
      "model_str_36": 1,
      "clip_str_36": 1,
      "lora_name_37": "None",
      "lora_wt_37": 1,
      "model_str_37": 1,
      "clip_str_37": 1,
      "lora_name_38": "None",
      "lora_wt_38": 1,
      "model_str_38": 1,
      "clip_str_38": 1,
      "lora_name_39": "None",
      "lora_wt_39": 1,
      "model_str_39": 1,
      "clip_str_39": 1,
      "lora_name_40": "None",
      "lora_wt_40": 1,
      "model_str_40": 1,
      "clip_str_40": 1,
      "lora_name_41": "None",
      "lora_wt_41": 1,
      "model_str_41": 1,
      "clip_str_41": 1,
      "lora_name_42": "None",
      "lora_wt_42": 1,
      "model_str_42": 1,
      "clip_str_42": 1,
      "lora_name_43": "None",
      "lora_wt_43": 1,
      "model_str_43": 1,
      "clip_str_43": 1,
      "lora_name_44": "None",
      "lora_wt_44": 1,
      "model_str_44": 1,
      "clip_str_44": 1,
      "lora_name_45": "None",
      "lora_wt_45": 1,
      "model_str_45": 1,
      "clip_str_45": 1,
      "lora_name_46": "None",
      "lora_wt_46": 1,
      "model_str_46": 1,
      "clip_str_46": 1,
      "lora_name_47": "None",
      "lora_wt_47": 1,
      "model_str_47": 1,
      "clip_str_47": 1,
      "lora_name_48": "None",
      "lora_wt_48": 1,
      "model_str_48": 1,
      "clip_str_48": 1,
      "lora_name_49": "None",
      "lora_wt_49": 1,
      "model_str_49": 1,
      "clip_str_49": 1
    },
    "class_type": "LoRA Stacker",
    "_meta": {
      "title": "LoRA Stacker"
    }
  },
  "43": {
    "inputs": {
      "control_net_name": "control_v11p_sd15_lineart_fp16.safetensors"
    },
    "class_type": "ControlNetLoaderAdvanced",
    "_meta": {
      "title": "Load Advanced ControlNet Model ğŸ›‚ğŸ…ğŸ…’ğŸ…"
    }
  },
  "44": {
    "inputs": {
      "strength": 0.65,
      "start_percent": 0,
      "end_percent": 0.85,
      "control_net": [
        "43",
        0
      ],
      "image": [
        "37",
        0
      ]
    },
    "class_type": "Control Net Stacker",
    "_meta": {
      "title": "Control Net Stacker"
    }
  },
  "48": {
    "inputs": {
      "preprocessor": "HEDPreprocessor",
      "resolution": 512,
      "image": [
        "15",
        0
      ]
    },
    "class_type": "AIO_Preprocessor",
    "_meta": {
      "title": "AIO Aux Preprocessor"
    }
  },
  "49": {
    "inputs": {
      "control_net_name": "control_v11p_sd15_softedge_fp16.safetensors"
    },
    "class_type": "ControlNetLoaderAdvanced",
    "_meta": {
      "title": "Load Advanced ControlNet Model ğŸ›‚ğŸ…ğŸ…’ğŸ…"
    }
  },
  "50": {
    "inputs": {
      "strength": 0.65,
      "start_percent": 0,
      "end_percent": 0.85,
      "control_net": [
        "49",
        0
      ],
      "image": [
        "48",
        0
      ],
      "cnet_stack": [
        "44",
        0
      ]
    },
    "class_type": "Control Net Stacker",
    "_meta": {
      "title": "Control Net Stacker"
    }
  },
  "52": {
    "inputs": {
      "control_net_name": "control_v11f1p_sd15_depth_fp16.safetensors"
    },
    "class_type": "ControlNetLoaderAdvanced",
    "_meta": {
      "title": "Load Advanced ControlNet Model ğŸ›‚ğŸ…ğŸ…’ğŸ…"
    }
  },
  "53": {
    "inputs": {
      "strength": 0.65,
      "start_percent": 0,
      "end_percent": 0.85,
      "control_net": [
        "52",
        0
      ],
      "image": [
        "55",
        0
      ],
      "cnet_stack": [
        "50",
        0
      ]
    },
    "class_type": "Control Net Stacker",
    "_meta": {
      "title": "Control Net Stacker"
    }
  },
  "55": {
    "inputs": {
      "ckpt_name": "depth_anything_vitl14.pth",
      "resolution": 512,
      "image": [
        "15",
        0
      ]
    },
    "class_type": "DepthAnythingPreprocessor",
    "_meta": {
      "title": "Depth Anything"
    }
  },
  "80": {
    "inputs": {
      "pixels": [
        "15",
        0
      ],
      "vae": [
        "38",
        4
      ]
    },
    "class_type": "VAEEncode",
    "_meta": {
      "title": "VAE Encode"
    }
  },
  "81": {
    "inputs": {
      "seed": 1037276997,
      "steps": 6,
      "cfg": 1.5,
      "sampler_name": "lcm",
      "scheduler": "sgm_uniform",
      "denoise": 0.9,
      "preview_method": "none",
      "vae_decode": "true",
      "model": [
        "94",
        0
      ],
      "positive": [
        "38",
        1
      ],
      "negative": [
        "38",
        2
      ],
      "latent_image": [
        "80",
        0
      ],
      "optional_vae": [
        "38",
        4
      ]
    },
    "class_type": "KSampler (Efficient)",
    "_meta": {
      "title": "KSampler (Efficient)"
    }
  },
  "83": {
    "inputs": {
      "model_name": "AnimateLCM_sd15_t2v.ckpt"
    },
    "class_type": "ADE_LoadAnimateDiffModel",
    "_meta": {
      "title": "Load AnimateDiff Model ğŸ­ğŸ…ğŸ…“â‘¡"
    }
  },
  "86": {
    "inputs": {
      "batch_offset": 0,
      "noise_type": "default",
      "seed_gen": "comfy",
      "seed_offset": 0,
      "adapt_denoise_steps": true
    },
    "class_type": "ADE_AnimateDiffSamplingSettings",
    "_meta": {
      "title": "Sample Settings ğŸ­ğŸ…ğŸ…“"
    }
  },
  "88": {
    "inputs": {
      "motion_model": [
        "83",
        0
      ]
    },
    "class_type": "ADE_ApplyAnimateDiffModelSimple",
    "_meta": {
      "title": "Apply AnimateDiff Model ğŸ­ğŸ…ğŸ…“â‘¡"
    }
  },
  "89": {
    "inputs": {
      "context_length": 16,
      "context_stride": 1,
      "context_overlap": 4,
      "closed_loop": false,
      "fuse_method": "pyramid",
      "use_on_equal_length": false,
      "start_percent": 0,
      "guarantee_steps": 1
    },
    "class_type": "ADE_LoopedUniformContextOptions",
    "_meta": {
      "title": "Context Optionsâ—†Looped Uniform ğŸ­ğŸ…ğŸ…“"
    }
  },
  "94": {
    "inputs": {
      "beta_schedule": "lcm >> sqrt_linear",
      "model": [
        "38",
        0
      ],
      "m_models": [
        "88",
        0
      ],
      "context_options": [
        "89",
        0
      ],
      "sample_settings": [
        "86",
        0
      ]
    },
    "class_type": "ADE_UseEvolvedSampling",
    "_meta": {
      "title": "Use Evolved Sampling ğŸ­ğŸ…ğŸ…“â‘¡"
    }
  },
  "111": {
    "inputs": {
      "frame_rate": 24,
      "loop_count": 0,
      "filename_prefix": "Minimalist",
      "format": "video/h264-mp4",
      "pix_fmt": "yuv420p",
      "crf": 19,
      "save_metadata": false,
      "pingpong": false,
      "save_output": true,
      "images": [
        "81",
        5
      ]
    },
    "class_type": "VHS_VideoCombine",
    "_meta": {
      "title": "Video Combine ğŸ¥ğŸ…¥ğŸ…—ğŸ…¢"
    }
  },
  "138": {
    "inputs": {
      "purge_cache": true,
      "purge_models": true,
      "anything": [
        "81",
        5
      ]
    },
    "class_type": "LayerUtility: PurgeVRAM",
    "_meta": {
      "title": "LayerUtility: Purge VRAM"
    }
  }
}